---
layout: post
title: wget
subtitle:
tags: [homework]
comments: false
---

Downloading all linked `txt` can also be achieved only with `wget` (no editor, regex-fu needed):

```bash
wget -r -l 1  --accept 'html,txt' 'https://univie-tnt-2019-fall.github.io/2019/L06/index.html'
```

Similarly, downloading the XML can also be done via a single command:

```bash
wget -O - "http://www.perseus.tufts.edu/hopper/collection?collection=Perseus%3Acollection%3ARichTimes" | awk '{ if (match($0,/href="(text[^"]*)"/,m)) print "http://www.perseus.tufts.edu/hopper/dl" m[1] }' | sort | wget -nc -i -
```

- This solution uses the "pipe" functionality of UNIX-shells (the `|`).
- The first `wget` pulls the html and prints it's contents to the standard output.
- The next command (`awk`) consumes ist and processes it line by line: if a line matches the regex, it prints the captured part of the match (here `m[1]`) and prepends a string. The result is a list of all XML URLs.
- After sorting (because: why not...?) the list of URLs is fed to `wget` again, this time reading from the standard input (symbolized in the command call by `-`).
- The whole command is abortable and resumable, because the second `wget` will not download already retrieved files (`-nc`).
